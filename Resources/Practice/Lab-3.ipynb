{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UQDQAQFzqB5e"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk import ne_chunk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "# Import the necessary libraries\n",
        "# add your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vL8cyoQgqU8M"
      },
      "outputs": [],
      "source": [
        "def ner(text):\n",
        "    # Download the necessary resources\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "    nltk.download('maxent_ne_chunker')\n",
        "    nltk.download('words')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VysbnTtnqcS8",
        "outputId": "f7897f46-1a30-444e-9e8f-537bf884c53d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'d', 'any', 'haven', 'all', 'the', 'ain', 'this', 'did', 'has', 'mustn', \"needn't\", 'then', \"couldn't\", \"isn't\", 'themselves', 'her', 'no', \"aren't\", 'was', 'or', 'hasn', 'against', 're', 't', 'does', 'how', 'own', 'our', 'ourselves', 'now', 'a', 'herself', \"mustn't\", \"hasn't\", 'your', 'while', 'have', 'doesn', 'out', 'into', 'yours', 'you', 'before', 'more', 'too', 'he', 'they', 'wouldn', 'do', 'can', \"haven't\", 'same', 'both', 'aren', 'so', 'll', \"you'd\", 'each', 'just', 'theirs', \"shouldn't\", 'what', 'were', 'having', 'yourself', 'is', 'had', 'won', 'under', 'been', 'she', 'if', 'yourselves', 'them', 'it', 'to', 'should', 'here', 'ma', 'during', 'm', 'on', \"won't\", \"mightn't\", 'are', 'than', 'shan', 'shouldn', 'by', 'between', 'needn', 'weren', 'as', 'up', 'i', \"that'll\", 'again', 'being', 'once', \"she's\", 'only', 'will', \"you've\", 'who', 'and', 'didn', 'over', 'mightn', \"you'll\", 'o', \"wouldn't\", \"you're\", 'myself', 've', 'very', 'above', 'y', 'down', 'wasn', 'my', \"doesn't\", 'its', 'an', 'don', 'hadn', \"didn't\", 'after', 'himself', 'most', 'because', 'which', 'hers', 'why', 'but', 'with', \"hadn't\", \"should've\", 'below', 'ours', \"shan't\", \"wasn't\", 'when', 'those', 'am', \"don't\", 'in', 'itself', 'me', 'him', 'whom', 'for', 'couldn', 'his', 'not', 's', 'doing', 'be', 'off', 'we', 'nor', 'such', 'isn', 'of', 'that', 'at', 'their', \"it's\", 'few', 'from', 'some', \"weren't\", 'other', 'until', 'further', 'through', 'where', 'about', 'there', 'these'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ],
      "source": [
        "text = \"Apple Inc. was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne. \" \\\n",
        "       \"The headquarters of Apple Inc. is located in Cupertino, California.\"\n",
        "ner(text)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7Cx_V0Rrd1O",
        "outputId": "7446a99c-d937-4ab8-cda8-7c84cb9c2ef9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Apple', 'Inc', '.', 'was', 'founded', 'by', 'Steve', 'Jobs', ',', 'Steve', 'Wozniak', ',', 'and', 'Ronald', 'Wayne', '.', 'The', 'headquarters', 'of', 'Apple', 'Inc', '.', 'is', 'located', 'in', 'Cupertino', ',', 'California', '.']\n"
          ]
        }
      ],
      "source": [
        "tokenizer = WordPunctTokenizer()\n",
        "filterdText = tokenizer.tokenize(text)\n",
        "print(filterdText)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "22eJanV2r139",
        "outputId": "68ebe0a8-8a54-4502-d816-e753a95b6d3f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "'WordPunctTokenizer' object is not iterable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-9c32178bd5f7>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'WordPunctTokenizer' object is not iterable"
          ]
        }
      ],
      "source": [
        "ner(text)\n",
        "\n",
        "result = [i for i in tokens if i not in stop_words]\n",
        "print (result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVyWQ10wsCUN",
        "outputId": "6726580f-b990-4ef9-bed4-10e8f15d2137"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hi NNP\n",
            ", ,\n",
            "This DT\n",
            "is VBZ\n",
            "Vikranth NNP\n",
            ", ,\n",
            "2022570 CD\n",
            "of IN\n",
            "IIITD NN\n",
            "! .\n",
            "[('Hi', 'NNP'), (',', ','), ('This', 'DT'), ('is', 'VBZ'), ('Vikranth', 'NNP'), (',', ','), ('2022570', 'CD'), ('of', 'IN'), ('IIITD', 'NN'), ('!', '.')]\n"
          ]
        }
      ],
      "source": [
        "tagged_words = pos_tag(tokens)\n",
        "for word, tag in tagged_words:\n",
        "    print(word, tag)\n",
        "print(tagged_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lwa1TedTtLkk"
      },
      "outputs": [],
      "source": [
        "ner_entities = ne_chunk(tagged_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQtj7AF6vTiy",
        "outputId": "08075bd0-8ffd-4dc0-f7ea-e9b17f3a26fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Hi', 'NNP'), (',', ','), ('This', 'DT'), ('is', 'VBZ'), ('Vikranth', 'NNP'), (',', ','), ('2022570', 'CD'), ('of', 'IN'), ('IIITD', 'NN'), ('!', '.')]\n"
          ]
        }
      ],
      "source": [
        "print(tagged_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJh6exQuvf0Z",
        "outputId": "169d8c15-caba-4d71-8695-e5b973e0be80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPE Hi\n",
            "PERSON Vikranth\n",
            "ORGANIZATION IIITD\n"
          ]
        }
      ],
      "source": [
        "for entity in ner_entities:\n",
        "    if hasattr(entity, 'label'):\n",
        "        print(entity.label(), ' '.join(c[0] for c in entity.leaves()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g91mcywsww4j"
      },
      "source": [
        "2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "5eu-8sdXwxmT",
        "outputId": "6a12f0e2-e226-4374-800f-3d07a5d026e7"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-84d492ac97de>\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m                               [0, -1, 0]])\n\u001b[1;32m     41\u001b[0m \u001b[0msharpened_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_sharpening\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0msharpened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgray\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgaussian_blurred_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;31m# Sharpening : For each pixel, subtract the gaussian blurred pixel intensity from 2 times the original pixel intensity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'gray' is not defined"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"cv.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1tUVV5uYrm48aul5URAHDdY3tIXzKm7gM\n",
        "\"\"\"\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "# Load the image\n",
        "image = cv2.imread('dog_input.jpg')\n",
        "# add code here\n",
        "\n",
        "# Check if the image was successfully loaded\n",
        "if image is None:\n",
        "    print('Error loading image.')\n",
        "    exit()\n",
        "\n",
        "# Convert the image to float\n",
        "image_float = image.astype(np.float32)\n",
        "\n",
        "# Mean blur\n",
        "mean_blurred_image = cv2.blur(image, ksize=(5, 5))\n",
        "# add code here\n",
        "\n",
        "# Median blur\n",
        "median_blurred_image = cv2.medianBlur(image, ksize=5)\n",
        "# add code here\n",
        "\n",
        "# Gaussian blur\n",
        "gaussian_blurred_image = cv2.GaussianBlur(image, (5, 5), 0)\n",
        "# add code here\n",
        "\n",
        "# Apply the sharpening formula\n",
        "kernel_sharpening = np.array([[0, -1, 0],\n",
        "                              [-1, 5, -1],\n",
        "                              [0, -1, 0]])\n",
        "sharpened_image = cv2.filter2D(image, -1, kernel_sharpening)\n",
        "sharpened = np.clip(2 * gray - gaussian_blurred_image, 0, 255).astype(np.uint8)\n",
        "# Sharpening : For each pixel, subtract the gaussian blurred pixel intensity from 2 times the original pixel intensity\n",
        "\n",
        "# add code here\n",
        "\n",
        "# Clip the pixel values to the valid range\n",
        "sharpened_image = np.clip(sharpened_image, 0, 255)\n",
        "\n",
        "# Convert the image back to uint8\n",
        "sharpened_image = sharpened_image.astype(np.uint8)\n",
        "\n",
        "# Edge detection (using sharpen and blur)\n",
        "# Edges : for each pixel, subtract the gaussian blurred pixel intensity from the original pixel intensity\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "edge_image = np.clip(gray - gaussian_blurred_image, 0, 255).astype(np.uint8)\n",
        "# add code here\n",
        "\n",
        "# Invert image\n",
        "# Hint - The maximum intensity of a pixel is 255 and minimum is 0. What operation can you do to invert a pixel's intensity?\n",
        "inverted_image = 255 - image\n",
        "# add code here\n",
        "\n",
        "# Display the original and transformed images\n",
        "cv2_imshow(image)\n",
        "cv2_imshow(mean_blurred_image)\n",
        "cv2_imshow(median_blurred_image)\n",
        "cv2_imshow(gaussian_blurred_image)\n",
        "cv2_imshow(sharpened_image)\n",
        "cv2_imshow(edge_image)\n",
        "cv2_imshow(inverted_image)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fM0cFwaW1_gn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
